{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimental Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpKyFQR9G5F8k2hmd/yxXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redher97/DheerajReddy_INFO5731_-Fall2021/blob/main/Sentimental_Analysis_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3poxXoLa__o"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "with open (r'C:\\Users\\reddy\\OneDrive\\Desktop\\final.csv','r', encoding = 'utf-8') as file:\n",
        "    df = pd.read_csv(file)\n",
        "df = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\whole_sentiment_analysis_roberta.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "df2 = pd.read_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\whole_sentiment_analysis_roberta.csv')\n",
        "mask = np.array(Image.open('twitter.png'))\n",
        "df = df2[df2['sentiment score']==1]\n",
        "\n",
        "comment_words = ''\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "\n",
        "for val in df.CleanedTweet:\n",
        "\tval = str(val)\n",
        "\ttokens = val.split()\n",
        "\tfor i in range(len(tokens)):\n",
        "\t\ttokens[i] = tokens[i].lower()\n",
        "\tcomment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(stopwords=STOPWORDS, \n",
        "               mask=mask, background_color=\"white\",\n",
        "               max_words=2000, max_font_size=256,\n",
        "               random_state=42, width=mask.shape[1],\n",
        "               height=mask.shape[0]).generate(comment_words)\n",
        "plt.figure(figsize = (20, 20), facecolor = None)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2s3T0mvXbR1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df2[df2['sentiment score']==2]\n",
        "\n",
        "comment_words = ''\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "\n",
        "for val in df.CleanedTweet:\n",
        "\tval = str(val)\n",
        "\ttokens = val.split()\n",
        "\tfor i in range(len(tokens)):\n",
        "\t\ttokens[i] = tokens[i].lower()\n",
        "\t\n",
        "\tcomment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(stopwords=STOPWORDS, \n",
        "               mask=mask, background_color=\"white\",\n",
        "               max_words=2000, max_font_size=256,\n",
        "               random_state=42, width=mask.shape[1],\n",
        "               height=mask.shape[0]).generate(comment_words)\n",
        "\n",
        "# plot the WordCloud image\t\t\t\t\t\n",
        "plt.figure(figsize = (20,20), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "isCai2FMbyKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,re\n",
        "import zipfile\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "punctuations= string.punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stopword_list = stopwords.words(\"english\")\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import FreqDist\n",
        "def clean_text(text,stop_words=True):\n",
        "    cleaned_text = text.lower() #preprocessing \n",
        "    cleaned_text = \"\".join(c for c in cleaned_text if c not in punctuations) #preprocessing \n",
        "    words = cleaned_text.split() \n",
        "    if stop_words==True:\n",
        "        words = [w for w in words if w not in stopword_list if w != '...'] #preprocessing \n",
        "        words = [w for w in words if w.isdigit() != True and w !='...']\n",
        "        words = [lem.lemmatize(word, \"v\") for word in words]\n",
        "        words = [lem.lemmatize(word, \"n\") for word in words] #preprocessing \n",
        "        cleaned_text = \" \".join(words)\n",
        "    return cleaned_text\n",
        "path = r\"C:/Users/reddy/OneDrive/Desktop/chiproject/2011-2021/\"\n",
        "directory = os.listdir(path)\n",
        "searchstring = 'Drugwar'\n",
        "fg=[]\n",
        "for fname in directory:\n",
        "    if os.path.isfile(path + os.sep + fname):\n",
        "        f = open(path + os.sep + fname, 'r')\n",
        "        if re.findall(searchstring,f.read()):\n",
        "            print(path+fname)\n",
        "            js=pd.read_json(path+fname)\n",
        "            df=pd.DataFrame([js['tweets'][i]['data'][j] for i in range(len(js['tweets'])) for j in range(len(js['tweets'][i]['data']))])\n",
        "            df = df.dropna(subset=['text'])\n",
        "            fg.append(df)\n",
        "        f.close()\n",
        "g=[]\n",
        "df3=fg[0]\n",
        "for i in range(len(fg)-1):\n",
        "    df3=fg[i+1][df3.columns]\n",
        "    g.append(df3)\n",
        "dt=pd.concat(fg,axis=0)\n",
        "dt['CleanedTweet']=dt['text'].apply(clean_text)\n",
        "dt['year']=pd.DatetimeIndex(dt['created_at']).year\n",
        "df=dt\n",
        "df.head()"
      ],
      "metadata": {
        "id": "d6nIr2u1bHF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.catplot(x='year',kind='count',data=df)"
      ],
      "metadata": {
        "id": "iBudrJbocbhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2011=df[df['year']==2011]\n",
        "df2012=df[df['year']==2012]\n",
        "df2013=df[df['year']==2013]\n",
        "df2014=df[df['year']==2014]\n",
        "df2015=df[df['year']==2015]\n",
        "df2016=df[df['year']==2016]\n",
        "df2017=df[df['year']==2017]\n",
        "df2018=df[df['year']==2018]\n",
        "df2019=df[df['year']==2019]\n",
        "df2020=df[df['year']==2020]\n",
        "df2021=df[df['year']==2021]"
      ],
      "metadata": {
        "id": "vnkE0eX1crTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2011 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2011.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "xmFQUH4mcy5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2012 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2012.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "Ya--4TK_c3vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2013 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2013.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "8m7Q1Rlyc4zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2014 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2014.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "ekyrd5bNc5eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2015 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2015.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "zzruaMnxc6CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2016 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2016.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "WpvrQFNbc6gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2017 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2017.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "D0iyKaqic68d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2018 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2018.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "p8k_3OIQc7YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2019 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2019.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "_g5KGacZc71K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2020 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2020.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "Nu_8GRyfc8RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import re\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install transformers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "'''# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\n",
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "#data_url = 'https://raw.githubusercontent.com/HuyenNguyenHelen/Drugwar/main/Top5%25.csv'\n",
        "#def loadFile (url):\n",
        "#  download = requests.get(url).content\n",
        "#  df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "#  print(df.shape)\n",
        "#  return df [['created_at', 'author_id','public_metricsretweet_count' , 'public_metricsreply_count', 'public_metricslike_count', 'public_metricsquote_count', 'CleanedTweet', 'hashtags extracted']]\n",
        "#df = loadFile (data_url)\n",
        "\n",
        "\n",
        "df2021 = df.dropna(subset=['CleanedTweet'])\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Loading the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')         #'cardiffnlp/twitter-roberta-base-sentiment')           #'nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained ('siebert/sentiment-roberta-large-english')   #('cardiffnlp/twitter-roberta-base-sentiment')               #('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "# Defining a sentiment detector\n",
        "def sentiment_score(tweet):\n",
        "    tokens = tokenizer.encode(tweet,\n",
        "                              max_length = 128,\n",
        "                              truncation=True,\n",
        "                              pad_to_max_length = True,\n",
        "                              return_tensors='pt')\n",
        "    # tokens.to(device)\n",
        "    result = model(tokens)\n",
        "    return int(torch.argmax(result.logits))+1\n",
        "\n",
        "i=0\n",
        "while i<(len(df)/10):\n",
        "    m=10*i\n",
        "    n=m+10\n",
        "    df_sub = df[m:n]\n",
        "    print(df_sub['CleanedTweet'])\n",
        "    df_sub['sentiment'] = df_sub['CleanedTweet'].apply(lambda x: sentiment_score(x))\n",
        "    df_sub.to_csv(r'C:\\Users\\reddy\\OneDrive\\Desktop\\2021.csv', mode='a', header=False)\n",
        "    print('{}-- done: {}-{}.....................'.format(i, m, n))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "-wOBWRJzc8sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df1 = pd.read_csv('/content/20'+'11'+'.csv',header=None)\n",
        "column=['n','lang', 'public_metrics', 'created_at', 'author_id', 'text', 'id',\n",
        "       'geo', 'withheld', 'CleanedTweet', 'year','sentiment']\n",
        "df1.columns=column\n",
        "fg=[df1]\n",
        "for i in range(12,22):\n",
        "  df2 = pd.read_csv('/content/20'+str(i)+'.csv')\n",
        "  df2.columns=column\n",
        "  fg.append(df2)\n",
        "df1=pd.concat(fg,axis=0)\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "XVzMOghScJVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.catplot(x='year',data=df1,kind='count',hue='sentiment',aspect=2)"
      ],
      "metadata": {
        "id": "KVZ46RzRcQob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}